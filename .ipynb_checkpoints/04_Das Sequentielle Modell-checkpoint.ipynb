{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Sequentielle Modell\n",
    "Dieses Notebook beschreibt das Sequentielle Modell, ein häufig verwendetes Modell für alle Arten von neuronalen Netzwerken.\n",
    "Zuerst wird der Aufbau erklärt, dann die zwei Phasen, in denen das Netzwerk erstens ein Ergebnis berechnet und zweitens lernt. Hierfür wird eine Implementierung eines CNN in Keras genutzt.\n",
    "## Aufbau\n",
    "Am Namen kann man ablesen, dass es um eine Folge geht. Die Folge, die hier gemeint ist, ist die der einzelnen Knoten im Netzwerk. Anhand folgender Abbildung, die schon in anderen Notebooks verwendet wurde, wird erklärt, was genau gemeint ist.\n",
    "<div style=\"width:70%;margin:auto\">\n",
    "<img src=\"assets/CNN.jpeg\" alt=\"Aufbau eines CNNs\"/>\n",
    "<small style=\"text-align:right;display:block\"><a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">Quelle</a></small>\n",
    "<div style=\"text-align:center;color:RGB(117,117,117)\">Aufbau eines Convolutional Neural Networks</div>\n",
    "</div>\n",
    "Jetzt liegt der Fokus auf der Richtung des Datenflusses, anschaulich die Verbindungslinien zwischen den Schichten. Bei dem Sequentiellen Modell muss gegeben sein, dass die Werte nur in eine Richtung fließen. Es darf also keine Verbindungslinien geben, die in eine andere als die nächste Schicht führen. Wenn man die Knoten und Kanten so anordnet, dann hat man ein Sequentielles Modell.\n",
    "<h2>Berechnungsphase (forward pass)</h2>\n",
    "Der forward pass beschreibt das, was im Notebook <a href=\"CNN - Einführung.ipynb\">Einführung</a> beschrieben wurde. Es werden alle Schichten nacheinander durchlaufen, wobei immer die Werte mit den Gewichten verrechnet werden. Die Schichten können entweder aus Knoten bestehen (vollständig oder teilweise verbunden), oder vor allem in CNNs Konvolutionsschichten sein. Die folgende Abbildung veranschaulicht den forward pass.\n",
    "<div style=\"width:70%;margin:auto\">\n",
    "<img src=\"assets/forward pass.png\" alt=\"Veranschaulichung des forward pass in einem simplen Neuronalen Netzwerk\"/>\n",
    "<small style=\"text-align:right;display:block\"><a href=\"https://data-science-blog.com/blog/2019/01/29/fehler-ruckfuhrung-mit-der-backpropagation/\">Quelle</a></small>\n",
    "<div style=\"text-align:center;color:RGB(117,117,117)\">Veranschaulichung des forward pass in einem simplen Neuronalen Netzwerk</div>\n",
    "</div>\n",
    "<h2>Anpassung der Gewichte (Backpropagation)</h2>\n",
    "<div class=\"alert alert-warning\">\n",
    "Für den Rest dieses Notebooks sind Kenntnisse der Backpropagation in Neuronalen Netzen hilfreich.</div>\n",
    "Nach einem forward pass wird eine Backpropagation durchgeführt. Diese sorgt dafür, dass die Gewichte angepasst werden und das Netzwerk lernt. Eine kurze Erklärung zur Funktionsweise: <br>\n",
    "Nach dem forward pass wird der Fehler zwischen dem Ergebnis des Netzwerks und dem tatsächlichen Wert berechnet. Dieser wird dann genutzt, um die Gewichte anzupassen. Die genaue Funktionsweise ist in folgender Abbildung dargestellt.\n",
    "<div style=\"width:70%;margin:auto\">\n",
    "<img src=\"assets/backpropagation.png\" alt=\"Veranschaulichung der Backpropagation in einem simplen Neuronalen Netzwerk\"/>\n",
    "<small style=\"text-align:right;display:block\"><a href=\"https://data-science-blog.com/blog/2019/01/29/fehler-ruckfuhrung-mit-der-backpropagation/\">Quelle</a></small>\n",
    "<div style=\"text-align:center;color:RGB(117,117,117)\">Veranschaulichung der Backpropagation in einem simplen Neuronalen Netzwerk</div>\n",
    "</div>\n",
    "Der Ausgangsknoten wird zuerst mit dem Fehler des Gesamtnetzwerkes angepasst. Die Gewichte, die in diesen Knoten führen, werden proportional zu ihrem Einfluss verändert. Zur Erklärung ein weiteres Beispiel. Es gibt einen Outputknoten und zwei Knoten im Layer davor.<br>\n",
    "Die Outputnode $o$ hat die Gewichte $o_1=0.25$ und $o_2=0.75$, die den Einfluss der beiden Nodes $1$ und $2$ im vorherigen Layer beschreiben. Jetzt wurde ein Fehler von $4$ gemessen, der zurück auf die Gewichte gemappt werden soll. Das Netzwerk hat eine Lernrate von $0.01$, was bedeutet, dass der Fehler erst auf die Nodes aufgeteilt wird und dann mit diesem Wert multipliziert wird. Also wird das Gewicht $o_1$ auf $0.25+\\frac{0.25}{0.25+0.75}*4*0.01=0.26$ und das Gewicht $o_2$ auf $0.75+\\frac{0.75}{0.25+0.75}*4*0.01=0.78$ geändert. Dann wird der Fehler, der für die Knoten in der drittletzten Schicht berechnet. Für die z. B. die Node $1$ ist dieser $\\frac{0.25}{0.25+0.75}*4=1$. Nachdem die Fehler für alle Nodes berechnet wurden, startet der Prozess wieder von vorne, nur dass jetzt Fehler von mehreren Nodes kommen. Diese werden aber einfach aufaddiert.\n",
    "<h3>Backpropagation in der Konvolutionsschicht</h3>\n",
    "Die Konvolution kann auch als zwei Schichten von Knoten ausgedrückt werden. Die blauen Knoten sind die Feature Map, die von einem 2D-Array auf ein 1D-Array umgeschlichtet wurde. Das gleiche wurde mit der rosa Output-Feature-Map getan. Jetzt kann die Konvolution als Teil eines normalen Neuronalen Netzwerks gesehen werden. Der einzige Unterschied ist, dass nicht alle Kanten vorhanden sind. Die Kanten werden durch den Kernel bestimmt und sind bunt markiert. Die folgende Abbildung zeigt unten die Konvolution und oben, wie sie als Neuronales Netzwerk aussehen würde. Zusätzlich wird rechts die gleiche Transformation mit dem Pooling veranschaulicht.\n",
    "<div style=\"width:60%;margin:auto\">\n",
    "<img src=\"assets/cnn backpropagation.png\" alt=\"Veranschaulichung der Konvolution als Neuronales Netzwerk\"/>\n",
    "<small style=\"text-align:right;display:block\"><a href=\"https://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\">Quelle</a></small>\n",
    "<div style=\"text-align:center;color:RGB(117,117,117)\">Veranschaulichung der Konvolution als Neuronales Netzwerk</div>\n",
    "</div>\n",
    "Es soll hier auch nicht tiefer ins Detail gegangen werden, da es sonst den Umfang dieses Notebooks sprengen würde. Eine gute Quelle für weitere Recherche zu diesem Thema ist <a href=\"https://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/\">hier</a>.\n",
    "<h3>Backpropagation in der Pooling-Schicht</h3>\n",
    "In der Pooling-Schicht funktioniert die Backpropagation folgendermaßen. Wenn ein forward pass durchgeführt wird, dann wird eine Mask gespeichert, die die ausgewählten Pixel der feature map enthält. Dann werden in der Backpropagation auch nur die Gewichte der Pixel geändert, die einen Einfluss auf das Ergebnis hatten. Die folgende Abbildung zeigt dieses Masking.\n",
    "<div style=\"width:60%;margin:auto;margin-top:20px\">\n",
    "<img src=\"assets/pooling.gif\" alt=\"Veranschaulichung der Backpropagation im Pooling Layer\"/>\n",
    "<small style=\"text-align:right;display:block\"><a href=\"https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5\">Quelle</a></small>\n",
    "<div style=\"text-align:center;color:RGB(117,117,117)\">Veranschaulichung der Backpropagation im Pooling Layer</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie lernt das CNN?\n",
    "Eine oft gestellte Frage ist, wie das Netzwerk durch den Wechsel von Konvolution und Pooling Features aus den Inputbildern extrahieren kann.\n",
    "### Pooling\n",
    "Pooling ist hier einfacher zu erklären. Obwohl durch das Layer auch Informationen gewonnen werden (mehr dazu später bei der Erklärung der Layer), führt Pooling größtenteils dazu, dass sich die Menge an Daten verringert. Bei einem $2x2$-Kernel sind das z. B. $75\\%$. Es geht dabei **keine wesentliche Information verloren**! Das Training wird jedoch durch die geringere Menge an Daten sehr beschleunigt.\n",
    "### Konvolution\n",
    "Bei der Konvolution läuft der Großteil der Informationsgewinnung ab. \n",
    "1. Im Startzustand sind die Kernel zufällig initialisiert\n",
    "1. Es wird ein Forward Pass durchgeführt und der Loss berechnet. Dieser ist zu Beginn noch sehr groß, dass Netzwerk hat noch nichts gelernt und kann deswegen die Klasse nur raten\n",
    "1. Die Backpropagation wird bis zu einem beispielhaften Konvolutionslayer durchgeführt und der Loss für diesen berechnet\n",
    "1. Der Loss wird auf den Kernel aufgeteilt, proportional zu dem Kernelwert an einem bestimmten Pixel\n",
    "1. Der Kernelwert an jedem Pixel wird so verändert, dass der zuvor berechnete Loss kleiner wird, wenn man den selben Forward Pass erneut durchführen würde\n",
    "1. => Das Netzwerk kann die in diesem Forward Pass enthaltenen Daten besser klassifizieren\n",
    "\n",
    "Dieser Prozess wird so lange wiederholt, bis alle Inputbilder einmal in einem Forward Pass enthalten waren. Dann ist das Ende einer Epoche erreicht. Jetzt haben alle Kernel für jedes Bild einen kleinen Schritt in die Richtung getan, die den Loss minimiert, also die Genauigkeit des Netzwerks erhöht. Da die häufig vorkommenden Merkmale öfter in die Updates der Kernel eingeflossen sind, werden die Kernel diese deutlicher zeigen als seltenere Merkmale. Die seltenen Merkmale heben sich außerdem teilweise gegenseitig auf, sodass nur die wesentlichen Features durch die Kernel abgebildet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen\n",
    "https://victorzhou.com/blog/keras-cnn-tutorial/<br>\n",
    "https://deeplizard.com/learn/video/6vweQjouLEE<br>\n",
    "https://victorzhou.com/blog/intro-to-cnns-part-2/<br>\n",
    "https://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
