{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer - SGD\n",
    "SGD steht für Stochastic Gradient Descent und ist ein Optimizer der genau für diesen Anwendungsfall gedacht ist. Normaler Stochastischer Gradientenabstieg konvergiert sehr langsam, da für jeden Input das gesamte Netzwerk vor- und rückwärts durchlaufen werden muss, und dann sehr kleine Schritte getan werden. Das soll durch den Optimizer SGD oder ausführlich SGD with momentum verbessert werden.<br>\n",
    "## Berechnung\n",
    "Wie bei RMSProp fließt ein exponentieller Mittelwert in die Berechnung des Gradientenupdates ein. In diesem Fall wird der Mittelwert jedoch nicht aus dem Quadrat des Gradienten gebildet, sondern aus dem Gradienten selbst. Dieser Faktor hat zur Auswirkung, dass der Algorithmus bildlich gesprochen Schwung aufbaut und größere Schritte tätigt.\n",
    "## Vorteile\n",
    "- Großer Geschwindigkeitsboost für Stochastischen Gradientenabstieg\n",
    "\n",
    "## Nachteile\n",
    "- SGD unterstützt keine Batches -> langsamer als andere Optimizer\n",
    "\n",
    "## Quellen\n",
    "https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
