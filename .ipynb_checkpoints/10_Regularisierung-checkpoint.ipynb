{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisierung\n",
    "Dieses Notebook erklärt die beiden Regularisierer L1 und L2. Sie werden dazu eingesetzt, um Overfitting zu vermeiden. Im Netzwerk werden sie zu der Kostenfunktion als weiterer Faktor hinzugefügt. Somit gibt es dann zu dem Loss-Faktor in der Kostenfunktion noch einen Faktor, der die Komplexität des Netzwerks darstellt. Die beispielhafte Kostenfunktion $$C_{MSE}=\\sum_j{(\\hat{y}_j-y_j)^2}$$ \n",
    "wird um einen Regularisierungsterm $R$ erweitert, der sich aber zwischen dem L1 und dem L2 Regularisierer unterscheidet$$C_{MSE,\\space reg.}=\\sum_j{(\\hat{y}_j-y_j)^2}+R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularisierer\n",
    "Der L1 Reguarisierer ist gegeben durch $$L1=\\sum_i{\\lvert w_i\\rvert}$$\n",
    "$w_i$ sind die Gewichte des Layers.<br>\n",
    "Der L1 Regularisierer sorgt also dafür, dass große Werte für die Gewichte bestraft werden (das Ziel des Trainings ist die Minimierung der Kostenfunktion). Weitergedacht werden die Gewichte, die nur für einen sehr kleinen Anteil an dem Ergebnis verantwortlich sind, sehr klein, weil ihr Nutzen den konstanten Faktor des L1 Regularisierers nicht überwinden kann.<br>\n",
    "Das ist der Hauptgrund, wofür dieser Regularisierer einesetzt wird. In Systemen mit hochdimensionalen Daten, die nicht voneinander abhängen, ist oft der erste Schritt des Machine Learnings die Reduktion des Feature Spaces (die Anzahl der Datendimensionen). Dafür sollen Dimensionen mit geringem Einfluss aus den Inputdaten entfernt werden. Das ist exakt die Auswirkung des L1 Regularisieres, weswegen er häufig für diese Aufgabe genutzt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularisierer\n",
    "Die Formel für den L2 Regularisierer ist $$L2=\\sum_i{(w_i)^2}$$\n",
    "Diese Regularisierung kann immer eingesetzt werden, um die Generalisierungsfähigkeit des Netzwerks zu erhöhen, während oben bei L1 auf Abhängigkeiten zwischen Features geachtet werden musste.<br>\n",
    "Durch den quadratischen Einfluss der Gewichte werden kleine Gewichte weniger von dem Regularisierer beeinflusst als größere. Das gilt für hohe positive aber auch negative Gewichte, der Absolutwert von Ausreißern wird verringert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierungsdetails\n",
    "Bei beiden Regularisierern wird ein neuer Parameter erzeugt, der jedoch in den allermeisten Fällen auf dem Standardwert gelassen werden kann. Dieser Wert $\\lambda$ beschreibt die Stärke der Regularisierung (meist $\\lambda=0.01$). Der Regularisierungsterm $R$ ist also $$R=\\lambda L$$$$L\\in L1,L2$$\n",
    "Und die Kostenfunktionen sind\n",
    "$$C_{MSE,\\space L1}=\\sum_j{(\\hat{y}_j-y_j)^2}+\\lambda \\sum_i{\\lvert w_i\\rvert}$$\n",
    "$$C_{MSE,\\space L2}=\\sum_j{(\\hat{y}_j-y_j)^2}+\\lambda \\sum_i{(w_i)^2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
