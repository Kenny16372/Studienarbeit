{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3343b8",
   "metadata": {},
   "source": [
    "# Optimizer - Adagrad\n",
    "Der erste bespochene Optimizer ist Adagrad. Der Name steht für Adaptive Gradient, zu deutsch angepasster Gradient. Die Anpassung bezieht sich in diesem Fall auf die einzelnen Dimensionen des Inputs. Für jede Inputdimension wird eine getrennte Lernrate (Schrittweite) verwendet, sodass in Richtung von Dimensionen mit geringerer Änderung größere Schritte getan werden als in Richtung sich stark ändernder Dimensionen.\n",
    "## Berechnung der Lernrate\n",
    "Die Anpassung der Lernrate je nach Inputdimension wird so durchgefüht, dass Die Lernrate für eine Dimension durch die Summe der Quadrate aller vorherigen Gradienten dieser Dimension geteilt wird.\n",
    "## Vorteile\n",
    "- Erhöhte Flexibilität in den Wertebereichen von Inputdimensionen\n",
    "- Gute Ergebnisse in Problemen mit wenigen, auf einem großen Bereich verteilten (sparse) Daten\n",
    "\n",
    "## Nachteile\n",
    "- Lernrate bleibt als Parameter erhalten\n",
    "- Oft konvergiert die angepasste Lernrate zu schnell gegen $0$ -> Der Algorithmus bleibt auf dem Weg zum Minimum stecken\n",
    "\n",
    "## Quellen\n",
    "https://machinelearningjourney.com/index.php/2021/01/05/adagrad-optimizer/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
